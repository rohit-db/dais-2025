{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakebase Data Querying & Analysis\n",
    "\n",
    "This notebook demonstrates how to query and analyze data from your Lakebase database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Configuration\n",
    "INSTANCE_NAME = \"rb-demo-lakebase\"\n",
    "PROFILE = \"az-demo\"\n",
    "USER_NAME = \"rohit.bhagwat@databricks.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get instance and generate credentials\n",
    "w = WorkspaceClient(profile=PROFILE)\n",
    "instance = w.database.get_database_instance(name=INSTANCE_NAME)\n",
    "cred = w.database.generate_database_credential(\n",
    "    request_id=str(uuid.uuid4()), \n",
    "    instance_names=[INSTANCE_NAME]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Connected to instance: {instance.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=instance.read_write_dns,\n",
    "    dbname=\"databricks_postgres\",\n",
    "    user=USER_NAME,\n",
    "    password=cred.token,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connected to PostgreSQL database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all coffee shops\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM coffee_operations.coffee_shops\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "print(f\"üìä Found {len(rows)} coffee shops\")\n",
    "print(\"\\nFirst 3 shops:\")\n",
    "for i, row in enumerate(rows[:3]):\n",
    "    print(f\"  {i+1}. {row[1]} - {row[2]}, {row[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shops by country\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT country, COUNT(*) as shop_count, AVG(seating_capacity) as avg_capacity\n",
    "        FROM coffee_operations.coffee_shops\n",
    "        GROUP BY country\n",
    "        ORDER BY shop_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    country_stats = cur.fetchall()\n",
    "    \n",
    "print(\"üåç Shops by Country:\")\n",
    "for country, count, avg_cap in country_stats:\n",
    "    print(f\"   {country}: {count} shops, avg {avg_cap:.0f} seats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premium vs Standard Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare premium vs standard locations\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            is_premium_location,\n",
    "            COUNT(*) as shop_count,\n",
    "            AVG(seating_capacity) as avg_capacity,\n",
    "            MIN(seating_capacity) as min_capacity,\n",
    "            MAX(seating_capacity) as max_capacity\n",
    "        FROM coffee_operations.coffee_shops\n",
    "        GROUP BY is_premium_location\n",
    "        ORDER BY is_premium_location DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    premium_stats = cur.fetchall()\n",
    "    \n",
    "print(\"‚≠ê Premium vs Standard Analysis:\")\n",
    "for is_premium, count, avg_cap, min_cap, max_cap in premium_stats:\n",
    "    type_label = \"Premium\" if is_premium else \"Standard\"\n",
    "    print(f\"\\n{type_label} Locations ({count} shops):\")\n",
    "    print(f\"   Average capacity: {avg_cap:.1f} seats\")\n",
    "    print(f\"   Capacity range: {min_cap} - {max_cap} seats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Zone Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shops by time zone\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            time_zone,\n",
    "            COUNT(*) as shop_count,\n",
    "            STRING_AGG(shop_name, ', ' ORDER BY shop_name) as shop_names\n",
    "        FROM coffee_operations.coffee_shops\n",
    "        GROUP BY time_zone\n",
    "        ORDER BY shop_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    tz_stats = cur.fetchall()\n",
    "    \n",
    "print(\"üïê Shops by Time Zone:\")\n",
    "for tz, count, names in tz_stats:\n",
    "    print(f\"\\n{tz} ({count} shops):\")\n",
    "    print(f\"   {names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Pandas for Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data into pandas DataFrame\n",
    "query = \"\"\"\n",
    "    SELECT shop_id, shop_name, city, state_province, country, \n",
    "           time_zone, latitude, longitude, seating_capacity, \n",
    "           is_premium_location, is_active, created_at\n",
    "    FROM coffee_operations.coffee_shops\n",
    "    ORDER BY shop_name\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(f\"üìä Loaded {len(df)} rows into DataFrame\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìà Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüåç Geographic Coverage:\")\n",
    "print(f\"   Countries: {df['country'].nunique()}\")\n",
    "print(f\"   Cities: {df['city'].nunique()}\")\n",
    "print(f\"   Time Zones: {df['time_zone'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Query session complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You've Accomplished\n",
    "\n",
    "‚úÖ **Connected** to Lakebase PostgreSQL database  \n",
    "‚úÖ **Explored** coffee shop data across multiple countries  \n",
    "‚úÖ **Analyzed** premium vs standard location patterns  \n",
    "‚úÖ **Examined** geographic and time zone distributions  \n",
    "‚úÖ **Loaded** data into pandas for advanced analytics  \n",
    "\n",
    "This demonstrates how Lakebase provides both operational database capabilities and analytical querying power in one unified platform!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language_type": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
