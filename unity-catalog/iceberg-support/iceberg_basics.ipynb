{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82921a4",
   "metadata": {},
   "source": [
    "# Unity Catalog and Iceberg Demo with Databricks Connect\n",
    "\n",
    "This notebook demonstrates how to use Databricks Connect to work with Apache Iceberg tables in Unity Catalog. We will use the `samples.nyctaxi.trips` dataset.\n",
    "\n",
    "**Steps:**\n",
    "1.  Initialize Databricks Connect.\n",
    "2.  Set up the catalog and schema.\n",
    "3.  Read data from the source table (`samples.nyctaxi.trips`).\n",
    "4.  Create a new managed Iceberg table.\n",
    "5.  Perform schema evolution.\n",
    "6.  Use time travel to query historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4ce96",
   "metadata": {},
   "source": [
    "## 1. Initialize Databricks Connect\n",
    "\n",
    "This assumes you have configured your environment for Databricks Connect. Typically, this involves:\n",
    "- Running `databricks configure` and providing your host (`https://e2-demo-field-eng.cloud.databricks.com/`) and a token.\n",
    "- To use a serverless SQL warehouse, set the `DATABRICKS_CLUSTER_ID` environment variable to your warehouse ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5f3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatabricksSession initialized successfully.\n",
      "Spark version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Initialize the DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "print(\"DatabricksSession initialized successfully.\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9d7f5",
   "metadata": {},
   "source": [
    "## 2. Setup Catalog and Schema\n",
    "\n",
    "Let's define a catalog and schema for our demo assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e7c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog 'rohitb_dais2025_demo' and schema 'iceberg_demo'.\n"
     ]
    }
   ],
   "source": [
    "catalog_name = \"rohitb_dais2025_demo\"\n",
    "schema_name = \"iceberg_demo\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"Using catalog '{catalog_name}' and schema '{schema_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7308a",
   "metadata": {},
   "source": [
    "## 3. Read from Source Table\n",
    "\n",
    "We'll read from the `samples.nyctaxi.trips` table provided by Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022803b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_zip</th>\n",
       "      <th>dropoff_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-13 21:47:53</td>\n",
       "      <td>2016-02-13 21:57:15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10103</td>\n",
       "      <td>10110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-13 18:29:09</td>\n",
       "      <td>2016-02-13 18:37:23</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10023</td>\n",
       "      <td>10023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-06 19:40:58</td>\n",
       "      <td>2016-02-06 19:52:32</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10001</td>\n",
       "      <td>10018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-12 19:06:43</td>\n",
       "      <td>2016-02-12 19:20:54</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.5</td>\n",
       "      <td>10044</td>\n",
       "      <td>10111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-23 10:27:56</td>\n",
       "      <td>2016-02-23 10:58:33</td>\n",
       "      <td>2.60</td>\n",
       "      <td>18.5</td>\n",
       "      <td>10199</td>\n",
       "      <td>10022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, trip_distance: double, fare_amount: double, pickup_zip: int, dropoff_zip: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_table = \"samples.nyctaxi.trips\"\n",
    "source_df = spark.read.table(source_table)\n",
    "\n",
    "# Display a few rows to inspect the data\n",
    "display(source_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52812e1a",
   "metadata": {},
   "source": [
    "## 4. Create and Load a Managed Iceberg Table\n",
    "\n",
    "Now, we will create a new Iceberg table using a subset of the columns from the source table. We'll also limit the data to 10,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14790720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and loaded Iceberg table: rohitb_dais2025_demo.iceberg_demo.nyc_taxi_trips_iceberg\n"
     ]
    }
   ],
   "source": [
    "iceberg_table_name = \"nyc_taxi_trips_iceberg\"\n",
    "\n",
    "# Select a subset of columns and data\n",
    "taxi_df = source_df.limit(10000)\n",
    "\n",
    "# Write the data to a new managed Iceberg table\n",
    "taxi_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(iceberg_table_name)\n",
    "\n",
    "print(f\"Created and loaded Iceberg table: {catalog_name}.{schema_name}.{iceberg_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0f7e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_zip</th>\n",
       "      <th>dropoff_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-13 21:47:53</td>\n",
       "      <td>2016-02-13 21:57:15</td>\n",
       "      <td>1.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10103</td>\n",
       "      <td>10110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-13 18:29:09</td>\n",
       "      <td>2016-02-13 18:37:23</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10023</td>\n",
       "      <td>10023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-06 19:40:58</td>\n",
       "      <td>2016-02-06 19:52:32</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10001</td>\n",
       "      <td>10018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-12 19:06:43</td>\n",
       "      <td>2016-02-12 19:20:54</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.5</td>\n",
       "      <td>10044</td>\n",
       "      <td>10111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-23 10:27:56</td>\n",
       "      <td>2016-02-23 10:58:33</td>\n",
       "      <td>2.60</td>\n",
       "      <td>18.5</td>\n",
       "      <td>10199</td>\n",
       "      <td>10022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-02-13 00:41:43</td>\n",
       "      <td>2016-02-13 00:46:52</td>\n",
       "      <td>1.40</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10023</td>\n",
       "      <td>10069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-02-18 23:49:53</td>\n",
       "      <td>2016-02-19 00:12:53</td>\n",
       "      <td>10.40</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11371</td>\n",
       "      <td>10003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-02-18 20:21:45</td>\n",
       "      <td>2016-02-18 20:38:23</td>\n",
       "      <td>10.15</td>\n",
       "      <td>28.5</td>\n",
       "      <td>11371</td>\n",
       "      <td>11201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-02-03 10:47:50</td>\n",
       "      <td>2016-02-03 11:07:06</td>\n",
       "      <td>3.27</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10014</td>\n",
       "      <td>10023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-02-19 01:26:39</td>\n",
       "      <td>2016-02-19 01:40:01</td>\n",
       "      <td>4.42</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10003</td>\n",
       "      <td>11222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, trip_distance: double, fare_amount: double, pickup_zip: int, dropoff_zip: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the new table to verify\n",
    "display(spark.table(iceberg_table_name).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844d8384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tpep_pickup_datetime</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tpep_dropoff_datetime</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trip_distance</td>\n",
       "      <td>double</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fare_amount</td>\n",
       "      <td>double</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pickup_zip</td>\n",
       "      <td>int</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dropoff_zip</td>\n",
       "      <td>int</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last_updated_ts</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td># Delta Statistics Columns</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Column Names</td>\n",
       "      <td>col-4, col-1, col-5, col-2, col-6, col-3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Column Selection Method</td>\n",
       "      <td>first-32</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td># Delta Uniform Iceberg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Metadata location</td>\n",
       "      <td>s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/6df43aae-d1b8-4ce9-90e2-5324b2bf8668/_iceberg/metadata/00001-10f6705b-fc2e-4f01-bbed-f1b27fc05f3f.gz.metadata.json</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Converted delta version</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Converted delta timestamp</td>\n",
       "      <td>1750541119692</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td># Detailed Table Information</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Catalog</td>\n",
       "      <td>rohitb_dais2025_demo</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Database</td>\n",
       "      <td>iceberg_demo</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(f\"describe table extended {iceberg_table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db20937",
   "metadata": {},
   "source": [
    "## 5. Schema Evolution\n",
    "\n",
    "Let's add a new column `last_updated_ts` to track when the record was last modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4240dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema evolved. Added column 'last_updated_ts'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tpep_pickup_datetime</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tpep_dropoff_datetime</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trip_distance</td>\n",
       "      <td>double</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fare_amount</td>\n",
       "      <td>double</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pickup_zip</td>\n",
       "      <td>int</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dropoff_zip</td>\n",
       "      <td>int</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last_updated_ts</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"ALTER TABLE {iceberg_table_name} ADD COLUMN last_updated_ts TIMESTAMP\")\n",
    "\n",
    "print(\"Schema evolved. Added column 'last_updated_ts'.\")\n",
    "\n",
    "# Verify the new schema\n",
    "display(spark.sql(f\"DESCRIBE TABLE {iceberg_table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97f275",
   "metadata": {},
   "source": [
    "## 6. Time Travel\n",
    "\n",
    "We can query the table's history and view the data as it was before the schema change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9a5413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-21 21:25:19.692</td>\n",
       "      <td>3124585922042689</td>\n",
       "      <td>rohit.bhagwat@databricks.com</td>\n",
       "      <td>ADD COLUMNS</td>\n",
       "      <td>{'columns': '[{\"column\":{\"name\":\"last_updated_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}}]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0621-212228-40wysa0e-v2n</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/16.4.x-photon-scala2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-21 21:24:58.004</td>\n",
       "      <td>3124585922042689</td>\n",
       "      <td>rohit.bhagwat@databricks.com</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'partitionBy': '[]', 'clusterBy': '[]', 'description': None, 'isManaged': 'true', 'properties': '{\"delta.enableIcebergCompatV2\":\"true\",\"write.metadata.path\":\"s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/6df43aae-d1b8-4ce9-90e2-5324b2bf8668/_iceberg/metadata\",\"delta.universalFormat.enabledFormats\":\"iceberg\",\"write.parquet.compression-codec\":\"zstd\",\"delta.enableIcebergWriterCompatV1\":\"true\",\"write.summary.partition-limit\":\"100\",\"write.wap.enabled\":\"false\",\"delta.enableTypeWidening\":\"true\",\"write.metadata.compression-codec\":\"gzip\",\"delta.checkpointPolicy\":\"v2\",\"write.object-storage.enabled\":\"true\",\"delta.columnMapping.mode\":\"id\",\"delta.columnMapping.maxColumnId\":\"6\",\"history.expire.min-snapshots-to-keep\":\"100\",\"write.data.path\":\"s3://databricks-e2demofieldengwest/b169b504-4c54-49f2-bc3a-adf4b128f36d/tables/6df43aae-d1b8-4ce9-90e2-5324b2bf8668\",\"delta.enablemanagedicebergtable\":\"true\",\"history.expire.max-snapshot-age-ms\":\"0\",\"gc.enabled\":\"false\",\"delta.enableInCommitTimestamps\":\"true\"}', 'statsOnLoad': 'true'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0621-212228-40wysa0e-v2n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WriteSerializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numFiles': '1', 'numRemovedFiles': '0', 'numRemovedBytes': '0', 'numOutputRows': '10000', 'numOutputBytes': '159058'}</td>\n",
       "      <td>None</td>\n",
       "      <td>Databricks-Runtime/16.4.x-photon-scala2.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the table history\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {iceberg_table_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2116a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of table at version 1:\n",
      "root\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_zip: integer (nullable = true)\n",
      " |-- dropoff_zip: integer (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table at version 1 (before the ALTER TABLE statement)\n",
    "df_v1 = spark.read.option(\"versionAsOf\", 1).table(iceberg_table_name)\n",
    "\n",
    "print(\"Schema of table at version 1:\")\n",
    "df_v1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15dd1a",
   "metadata": {},
   "source": [
    "Notice that the `last_updated_ts` column is not present in the schema of the first version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef53dd",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Run the following commands to remove the resources created in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e20fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"DROP TABLE IF EXISTS {iceberg_table_name}\")\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {schema_name}\")\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name}\")\n",
    "# print(\"Cleaned up resources.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
